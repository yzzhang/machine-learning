{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Object-Oriented Machine Learning Pipeline for Pandas and Koalas DataFrames\n",
    "\n",
    "## End-to-end process of developing Spark-enabled machine learning pipeline using Pandas, Koalas, Scikit-Learn, and mlflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the article [Python Data Preprocessing Using Pandas DataFrame, Spark DataFrame, and Koalas DataFrame](https://towardsdatascience.com/python-data-preprocessing-using-pandas-dataframe-spark-dataframe-and-koalas-dataframe-e44c42258a8f), I used a public dataset to evaluate and compare the basic functionality of Pandas, Spark, and Koalas DataFrames in typical data preprocessing steps for machine learning. \n",
    "\n",
    "In this article, I use the [Interview Attendance Problem for Kaggle competition](https://www.kaggle.com/vishnusraghavan/the-interview-attendance-problem) to demonstrate an end-to-end process of developing a machine learning pipeline for both Pandas and Koalas dataframes using Pandas, Koalas, Scikit-Learn, and mlflow. This is achieved by:\n",
    "* Developing a scikit-learn data preprocessing pipeline using Pandas with scikit-learn\n",
    "* Developing a scikit-learn data preprocessing pipeline for Spark by combining scikit-learn with Koalas\n",
    "* Developing a machine learning pipeline by combining scikit-learn with mlflow\n",
    "\n",
    "The end-to-end development process is based on the [Cross-industry standard process for data mining](https://en.wikipedia.org/wiki/Cross-industry_standard_process_for_data_mining). As shown in the diagram below, it consists of six major phases:\n",
    "* Business Understanding\n",
    "* Data Understanding\n",
    "* Data Preparation\n",
    "* Modeling\n",
    "* Evaluation\n",
    "* Deployment\n",
    "\n",
    "    <img src=\"./images/CRISP-DM_Process_Diagram.png\" width=\"180\" height=\"360\">\n",
    "\n",
    "Figure 1: CRISP-DM process diagram (refer to [source in Wikipedia](https://en.wikipedia.org/wiki/Cross-industry_standard_process_for_data_mining#/media/File:CRISP-DM_Process_Diagram.png))\n",
    "\n",
    "For convenience of discussion, it is assumed that the following Python libraries have been installed on a local machine such as Mac:\n",
    "* Anaconda (conda 4.7.10) with Python 3.6, Numpy, Pandas, Matplotlib, and Scikit-Learn\n",
    "* [pyspark 2.4.4](https://spark.apache.org/releases/spark-release-2-4-4.html)\n",
    "* [Koalas](https://github.com/databricks/koalas)\n",
    "* [mlflow](https://www.mlflow.org/docs/latest/index.html)\n",
    "\n",
    "## 1. Business Understanding\n",
    "\n",
    "The key point of business understanding is to understand the business problem to be solved. As an example, the following is a brief description of the Kaggle interview attendance problem: \n",
    "\n",
    "Given a set of questions that are asked by a recruiter while scheduling an interview with a candidate, how to use the answers to those questions from the candidate to determine whether the expected attendance is yes, no, or uncertain.\n",
    "\n",
    "## 2. Data Understanding\n",
    "\n",
    "Once the business problem is understood, the next step is to identify where (i.e., data sources) and how we can collect data from which a machine learning solution to the problem can be built. \n",
    "\n",
    "The labeled dataset for the Kaggle interview attendance problem has been collected as a csv file from the recruitment industry in India by the researchers over a period of more than 2 years between September 2014 and January 2017.  \n",
    "\n",
    "The following table shows the first five rows of the raw dataset. This dataset is collected for supervised machine learning and the column of Observed Attendance holds the labels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Environment\n",
    "\n",
    "* conda create -n python36 python=3.6 anaconda\n",
    "* conda activate python36\n",
    "* pip install koalas\n",
    "* pip install mlflow\n",
    "* pip install pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "colab_type": "code",
    "id": "VFA_n4HYbeL5",
    "outputId": "358ec79a-8cf8-426e-830f-ccbe9bebaa16"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yuhuang/anaconda3/envs/python36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: DeprecationWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.\n",
      "  warnings.warn(msg, category=DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import databricks.koalas as ks\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "from   datetime import datetime\n",
    "import os\n",
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dQ_HqJz1lclZ"
   },
   "source": [
    "The following code is to load the dataset csv file (e.g., Interview_Attendance_Data.csv) from your local machine into Koalas DataFrame. The dataset can be downloaded from Kaggle site: https://www.kaggle.com/vishnusraghavan/the-interview-attendance-problem/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date of Interview</th>\n",
       "      <th>Client name</th>\n",
       "      <th>Industry</th>\n",
       "      <th>Location</th>\n",
       "      <th>Position to be closed</th>\n",
       "      <th>Nature of Skillset</th>\n",
       "      <th>Interview Type</th>\n",
       "      <th>Name(Cand ID)</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Candidate Current Location</th>\n",
       "      <th>Candidate Job Location</th>\n",
       "      <th>Interview Venue</th>\n",
       "      <th>Candidate Native location</th>\n",
       "      <th>Have you obtained the necessary permission to start at the required time</th>\n",
       "      <th>Hope there will be no unscheduled meetings</th>\n",
       "      <th>Can I Call you three hours before the interview and follow up on your attendance for the interview</th>\n",
       "      <th>Can I have an alternative number/ desk number. I assure you that I will not trouble you too much</th>\n",
       "      <th>Have you taken a printout of your updated resume. Have you read the JD and understood the same</th>\n",
       "      <th>Are you clear with the venue details and the landmark.</th>\n",
       "      <th>Has the call letter been shared</th>\n",
       "      <th>Observed Attendance</th>\n",
       "      <th>Marital Status</th>\n",
       "      <th>_c22</th>\n",
       "      <th>_c23</th>\n",
       "      <th>_c24</th>\n",
       "      <th>_c25</th>\n",
       "      <th>_c26</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>13.02.2015</td>\n",
       "      <td>Hospira</td>\n",
       "      <td>Pharmaceuticals</td>\n",
       "      <td>Chennai</td>\n",
       "      <td>Production- Sterile</td>\n",
       "      <td>Routine</td>\n",
       "      <td>Scheduled Walkin</td>\n",
       "      <td>Candidate 1</td>\n",
       "      <td>Male</td>\n",
       "      <td>Chennai</td>\n",
       "      <td>Hosur</td>\n",
       "      <td>Hosur</td>\n",
       "      <td>Hosur</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>Single</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>13.02.2015</td>\n",
       "      <td>Hospira</td>\n",
       "      <td>Pharmaceuticals</td>\n",
       "      <td>Chennai</td>\n",
       "      <td>Production- Sterile</td>\n",
       "      <td>Routine</td>\n",
       "      <td>Scheduled Walkin</td>\n",
       "      <td>Candidate 2</td>\n",
       "      <td>Male</td>\n",
       "      <td>Chennai</td>\n",
       "      <td>Bangalore</td>\n",
       "      <td>Hosur</td>\n",
       "      <td>Trichy</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>Single</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>13.02.2015</td>\n",
       "      <td>Hospira</td>\n",
       "      <td>Pharmaceuticals</td>\n",
       "      <td>Chennai</td>\n",
       "      <td>Production- Sterile</td>\n",
       "      <td>Routine</td>\n",
       "      <td>Scheduled Walkin</td>\n",
       "      <td>Candidate 3</td>\n",
       "      <td>Male</td>\n",
       "      <td>Chennai</td>\n",
       "      <td>Chennai</td>\n",
       "      <td>Hosur</td>\n",
       "      <td>Chennai</td>\n",
       "      <td>NA</td>\n",
       "      <td>Na</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>No</td>\n",
       "      <td>Single</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>13.02.2015</td>\n",
       "      <td>Hospira</td>\n",
       "      <td>Pharmaceuticals</td>\n",
       "      <td>Chennai</td>\n",
       "      <td>Production- Sterile</td>\n",
       "      <td>Routine</td>\n",
       "      <td>Scheduled Walkin</td>\n",
       "      <td>Candidate 4</td>\n",
       "      <td>Male</td>\n",
       "      <td>Chennai</td>\n",
       "      <td>Chennai</td>\n",
       "      <td>Hosur</td>\n",
       "      <td>Chennai</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>Single</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>13.02.2015</td>\n",
       "      <td>Hospira</td>\n",
       "      <td>Pharmaceuticals</td>\n",
       "      <td>Chennai</td>\n",
       "      <td>Production- Sterile</td>\n",
       "      <td>Routine</td>\n",
       "      <td>Scheduled Walkin</td>\n",
       "      <td>Candidate 5</td>\n",
       "      <td>Male</td>\n",
       "      <td>Chennai</td>\n",
       "      <td>Bangalore</td>\n",
       "      <td>Hosur</td>\n",
       "      <td>Chennai</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>Married</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Date of Interview Client name         Industry Location Position to be closed Nature of Skillset    Interview Type Name(Cand ID) Gender Candidate Current Location Candidate Job Location Interview Venue Candidate Native location Have you obtained the necessary permission to start at the required time Hope there will be no unscheduled meetings Can I Call you three hours before the interview and follow up on your attendance for the interview Can I have an alternative number/ desk number. I assure you that I will not trouble you too much Have you taken a printout of your updated resume. Have you read the JD and understood the same Are you clear with the venue details and the landmark. Has the call letter been shared Observed Attendance Marital Status  _c22  _c23  _c24  _c25  _c26\n",
       "0        13.02.2015     Hospira  Pharmaceuticals  Chennai   Production- Sterile            Routine  Scheduled Walkin   Candidate 1   Male                    Chennai                  Hosur           Hosur                     Hosur                                                Yes                                                              Yes                                                Yes                                                                                                Yes                                                                                              Yes                                                                                            Yes                                 Yes                  No         Single  None  None  None  None  None\n",
       "1        13.02.2015     Hospira  Pharmaceuticals  Chennai   Production- Sterile            Routine  Scheduled Walkin   Candidate 2   Male                    Chennai              Bangalore           Hosur                    Trichy                                                Yes                                                              Yes                                                Yes                                                                                                Yes                                                                                              Yes                                                                                            Yes                                 Yes                  No         Single  None  None  None  None  None\n",
       "2        13.02.2015     Hospira  Pharmaceuticals  Chennai   Production- Sterile            Routine  Scheduled Walkin   Candidate 3   Male                    Chennai                Chennai           Hosur                   Chennai                                                 NA                                                               Na                                                 NA                                                                                                 NA                                                                                               NA                                                                                             NA                                  NA                  No         Single  None  None  None  None  None\n",
       "3        13.02.2015     Hospira  Pharmaceuticals  Chennai   Production- Sterile            Routine  Scheduled Walkin   Candidate 4   Male                    Chennai                Chennai           Hosur                   Chennai                                                Yes                                                              Yes                                                 No                                                                                                Yes                                                                                               No                                                                                            Yes                                 Yes                  No         Single  None  None  None  None  None\n",
       "4        13.02.2015     Hospira  Pharmaceuticals  Chennai   Production- Sterile            Routine  Scheduled Walkin   Candidate 5   Male                    Chennai              Bangalore           Hosur                   Chennai                                                Yes                                                              Yes                                                Yes                                                                                                 No                                                                                              Yes                                                                                            Yes                                 Yes                  No        Married  None  None  None  None  None"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_file = './data/Interview_Attendance_Data.csv'\n",
    "ks_df = ks.read_csv(dataset_file)\n",
    "ks_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Preparation\n",
    "\n",
    "The main goal of data preparation is to transform the raw dataset into appropriate format so that the transformed data can be effectively consumed by a target machine learning model. \n",
    "\n",
    "In the raw dataset, the column of Name(Cand ID)\tcontains candidate unique identifier, which does not have much prediction power and thus can be dropped. In addition, all of the columns (i.e., columns from _c22 to _c26 for Koalas dataframe, or columns from Unnamed: 22 to Unnamed: 26 for Pandas dataframe) have no data and thus can safely be dropped as well.\n",
    "\n",
    "Except for the date of interview, all of the other columns in the dataset have categorical (textual) values. In order to use machine learning to solve the problem, those categorical values must be transformed into numerical values because a machine learning model can only cosume numerical data. \n",
    "\n",
    "The column of Date of Interview should be splitted into Day, Month, and Year to increase prediction power since the information of individual Day and Month can be more coorelated with seasonable jobs compared with a string of date. \n",
    "\n",
    "The columns of Nature of Skillset and Candidate Native location have a large number of unique entries. These will introduce a large number of new derived features after one-hot encoding. Too many features can lead to a [curse of dimensionality](https://en.wikipedia.org/wiki/Curse_of_dimensionality) problem. To alleviate such problem, the values of these two columns are re-divided into a smaller number of buckets.\n",
    "\n",
    "The above data preprocessing/transformation can be summarized as following steps:\n",
    "* Bucketing skillset\n",
    "* Bucketing candidate native location\n",
    "* Parsing interview date\n",
    "* Changing categorical values to uppercase and dropping less useful features \n",
    "* One-Hot encoding categorical values\n",
    "\n",
    "These steps are implemented by developing an Object-Oriented data preprocessing pipeline for both Pandas and Koalas dataframes using Pandas, Koalas and scikit-learn pipeline API (i.e., BaseEstimator, TransformerMixin, Pipeline)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OqKw66XKdQbj"
   },
   "source": [
    "Main Goals:\n",
    "\n",
    "\n",
    "\n",
    "1. Create a model predicting if a candidate will attend an interview. This will be indicated by the \"Observed Attendance\" column in the data set. Create the model only using the records where this column is not null\n",
    "\n",
    "2. Provide a probability and a prediction for the candidates where the \"Observed Attendance\" column is null."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Transforming Column Values\n",
    "\n",
    "Several data preprocessing steps share a common operation of transforming the values of a particular column in a Koalas dataframe. But, as described in [Koalas Series](https://koalas.readthedocs.io/en/latest/reference/api/databricks.koalas.Series.iloc.html#databricks.koalas.Series.iloc), the Koalas Series does not support some of the common Pandas indexing mechanisms such as df.iloc[0]. Because of this, there is no simple method of traversing and changling the values of a column in a Koalas dataframe. The other difficulty is that Koalas does not allow to build a new Koalas Series object from scratch and then add it as a new column in an existing Koalas dataframe. It only allows to use a new Koalas Series object that is built from the existing columns of a Koalas dataframe. These difficulties are avoided by defining a global function to call the apply() method of a Koalas Series object. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transformColumn(column_values, func, func_type):\n",
    "    '''\n",
    "    This function is to transform a given column (column_values) of a Koalas DataFrame or Series.\n",
    "    This function is needed because the current Koalas requires that the applied function has \n",
    "    an explictly specified return type. Because of this, we cannot use lambda function directly \n",
    "    since lambda function does not have an explicit return type.\n",
    "    '''\n",
    "    def transform_column(column_element) -> func_type:\n",
    "        return func(column_element)\n",
    "    \n",
    "    cvalues = column_values\n",
    "        \n",
    "    cvalues = cvalues.apply(transform_column)\n",
    "            \n",
    "    return cvalues"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Bucketing Skillset\n",
    "\n",
    "To alleviate the curse of dimensionality issue, the transform() method of the BucketSkillset class divides the unique values of the skillset column into smaller number of buckets by combining those values that appear less than 9 times into one same value of Others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HKyE5zV4eSZo"
   },
   "outputs": [],
   "source": [
    "class BucketSkillset(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        '''\n",
    "        This class is to re-bucket the skill sets and candidates location features \n",
    "        to combine small catogaries into one catogary 'Others'.\n",
    "        '''\n",
    "        self.skillset = ['JAVA/J2EE/Struts/Hibernate', 'Fresher', 'Accounting Operations', 'CDD KYC', 'Routine', 'Oracle', \n",
    "          'JAVA/SPRING/HIBERNATE/JSF', 'Java J2EE', 'SAS', 'Oracle Plsql', 'Java Developer', \n",
    "          'Lending and Liabilities', 'Banking Operations', 'Java', 'Core Java', 'Java J2ee', 'T-24 developer', \n",
    "          'Senior software engineer-Mednet', 'ALS Testing', 'SCCM', 'COTS Developer', 'Analytical R & D', \n",
    "          'Sr Automation Testing', 'Regulatory', 'Hadoop', 'testing', 'Java', 'ETL', 'Publishing']       \n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "        '''\n",
    "        This method is to re-bucket the skill sets features.\n",
    "        '''\n",
    "        \n",
    "        func = lambda x: x if x in self.skillset else 'Others'\n",
    "               \n",
    "        X1 = X.copy()\n",
    "        \n",
    "        cname = 'Nature of Skillset'\n",
    "        cvalue = X1[cname]\n",
    "        \n",
    "        if type(X1) == ks.DataFrame:  \n",
    "            cvalue = transformColumn(cvalue, func, str)\n",
    "            X1[cname] = cvalue\n",
    "        elif type(X1) == pd.DataFrame:\n",
    "            X2 = map(func, cvalue)\n",
    "            X1[cname] = pd.Series(X2)\n",
    "        else:\n",
    "            print('BucketSkillset: unsupported dataframe: {}'.format(type(X1)))\n",
    "            pass\n",
    "            \n",
    "        return X1  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Bucketing candidate native location\n",
    "Similarly to bucketing skillset, to alleviate the curse of dimensionality issue, the transform() method of the BucketLocation class divides the unique values of the candidate native location column into smaller number of buckets by combining those values that appear less than 12 times into one same value of Others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BucketLocation(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        '''\n",
    "        This class is to re-bucket the candidates location features \n",
    "        to combine small catogaries into one catogary 'Others'.\n",
    "        '''\n",
    "        \n",
    "        self.candidate_locations = ['Chennai', 'Hyderabad', 'Bangalore', 'Gurgaon', 'Cuttack', 'Cochin', \n",
    "                          'Pune', 'Coimbatore', 'Allahabad', 'Noida', 'Visakapatinam', 'Nagercoil',\n",
    "                          'Trivandrum', 'Kolkata', 'Trichy', 'Vellore']\n",
    "        \n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "        '''\n",
    "        This method is to re-bucket the candidates native locations features.\n",
    "        '''\n",
    "            \n",
    "        X1 = X.copy()\n",
    "        \n",
    "        func = lambda x: x if x in self.candidate_locations else 'Others'\n",
    "        \n",
    "        cname = 'Candidate Native location'\n",
    "        cvalue = X1[cname]\n",
    "        if type(X1) == ks.DataFrame: \n",
    "            cvalue = transformColumn(cvalue, func, str)\n",
    "            X1[cname] = cvalue\n",
    "        elif type(X1) == pd.DataFrame:\n",
    "            X2 = map(func, cvalue)\n",
    "            X1[cname] = pd.Series(X2)\n",
    "        else:\n",
    "            print('BucketLocation: unsupported dataframe: {}'.format(type(X1)))\n",
    "            pass\n",
    "            \n",
    "        return X1  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Parsing Interview Date\n",
    "\n",
    "The values of the column of Date of Interview are messy in that various formats are used. For instance not only different delimits are used to separate day, month, and year, but also different orders of day, month, and year are followed. This is handled by the local \\_parseDate() and transform_date() methods of the ParseInterviewDate class.  The overall functionality of the transform() method is to separate the interview date string into individual day, month, and year values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tdbU3ZTTeHXp"
   },
   "outputs": [],
   "source": [
    "class ParseInterviewDate(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        '''\n",
    "        This class is to splits the date of interview into day (2 digits), month (2 digits), year (4 digits).\n",
    "        '''     \n",
    "    def __parseDate(self, string, delimit):\n",
    "        try:\n",
    "            if ('&' in string):\n",
    "                subs = tuple(string.split('&'))\n",
    "                string = subs[0]\n",
    "        except:\n",
    "            print ('TypeError: {}'.format(string))\n",
    "            return None\n",
    "        \n",
    "        string = string.strip()\n",
    "        \n",
    "        try:\n",
    "            d = datetime.strptime(string, '%d{0}%m{0}%Y'.format(delimit))\n",
    "        except:\n",
    "            try:\n",
    "                d = datetime.strptime(string, '%d{0}%m{0}%y'.format(delimit))\n",
    "            except:\n",
    "                try:\n",
    "                     d = datetime.strptime(string, '%d{0}%b{0}%Y'.format(delimit))\n",
    "                except:\n",
    "                    try:\n",
    "                         d = datetime.strptime(string, '%d{0}%b{0}%y'.format(delimit))\n",
    "                    except:\n",
    "                        try:\n",
    "                            d = datetime.strptime(string, '%b{0}%d{0}%Y'.format(delimit))\n",
    "                        except:\n",
    "                            try:\n",
    "                                d = datetime.strptime(string, '%b{0}%d{0}%y'.format(delimit))\n",
    "                            except:\n",
    "                                d = None\n",
    "        return d\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "        '''\n",
    "        This method splits the date of interview into day (2 digits), month (2 digits), year (4 digits).\n",
    "        '''\n",
    "        \n",
    "        def transform_date(ditem):\n",
    "            if (isinstance(ditem, str) and len(ditem) > 0):\n",
    "                if ('.' in ditem):\n",
    "                    d = self.__parseDate(ditem, '.')\n",
    "                elif ('/' in ditem):\n",
    "                    d = self.__parseDate(ditem, '/')\n",
    "                elif ('-' in ditem):\n",
    "                    d = self.__parseDate(ditem, '-')\n",
    "                elif (' ' in ditem):\n",
    "                    d = self.__parseDate(ditem, ' ')\n",
    "                else:\n",
    "                    d = None\n",
    "                    \n",
    "                if (d is None):\n",
    "                    return 0, 0, 0\n",
    "                else:\n",
    "                    return d.day, d.month, d.year\n",
    "                \n",
    "        def get_day(column_element) -> int:\n",
    "            try:\n",
    "                day, month, year = transform_date(column_element)\n",
    "                return int(day)\n",
    "            except:\n",
    "                return 0\n",
    "        \n",
    "        def get_month(column_element) -> int:\n",
    "            try:\n",
    "                day, month, year = transform_date(column_element)\n",
    "                return int(month)\n",
    "            except:\n",
    "                return 0\n",
    "        \n",
    "        def get_year(column_element) -> int:\n",
    "            try:\n",
    "                day, month, year = transform_date(column_element)\n",
    "                return int(year)\n",
    "            except:\n",
    "                return 0\n",
    "            \n",
    "        def pandas_transform_date(X1):\n",
    "            days = []\n",
    "            months = []\n",
    "            years = []\n",
    "            ditems = X1['Date of Interview'].values\n",
    "            for ditem in ditems:\n",
    "                if (isinstance(ditem, str) and len(ditem) > 0):\n",
    "                    if ('.' in ditem):\n",
    "                        d = self.__parseDate(ditem, '.')\n",
    "                    elif ('/' in ditem):\n",
    "                        d = self.__parseDate(ditem, '/')\n",
    "                    elif ('-' in ditem):\n",
    "                        d = self.__parseDate(ditem, '-')\n",
    "                    elif (' ' in ditem):\n",
    "                        d = self.__parseDate(ditem, ' ')\n",
    "                    else:\n",
    "                        d = None\n",
    "                    \n",
    "                    if (d is None):\n",
    "                        # print(\"{}, invalid format of interview date!\".format(ditem))\n",
    "                        days.append(0) # 0 - NaN\n",
    "                        months.append(0)\n",
    "                        years.append(0)\n",
    "                    else:\n",
    "                        days.append(d.day) \n",
    "                        months.append(d.month)\n",
    "                        years.append(d.year)\n",
    "                else:\n",
    "                    days.append(0)\n",
    "                    months.append(0)\n",
    "                    years.append(0)\n",
    "        \n",
    "            X1['Year'] = years\n",
    "            X1['Month'] = months\n",
    "            X1['Day'] = days\n",
    "            \n",
    "            return X1\n",
    "        \n",
    "        X1 = X.copy()\n",
    "        \n",
    "        if type(X1) == ks.DataFrame: \n",
    "            X1['Year'] = X1['Date of Interview']\n",
    "            X1['Month'] = X1['Date of Interview']\n",
    "            X1['Day'] = X1['Date of Interview']\n",
    "        \n",
    "            func_map = {'Year' : get_year, 'Month' : get_month, 'Day' : get_day}\n",
    "            for cname in func_map:\n",
    "                cvalue = X1[cname]\n",
    "                cvalue = cvalue.apply(func_map[cname])\n",
    "                X1[cname] = cvalue\n",
    "        elif type(X1) == pd.DataFrame:\n",
    "            X1 = pandas_transform_date(X1)\n",
    "        else:\n",
    "            print('ParseInterviewDate: unsupported dataframe: {}'.format(type(X1)))\n",
    "            pass \n",
    "         \n",
    "        return X1   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 Changing Categorical Values to Uppercase and Dropping Less Useful Features\n",
    "\n",
    "The transform() method of the FeaturesUppercase class is to change the values of categorical features to uppercase and at the same time drop less useful features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "s2N0ZOB3d8mh"
   },
   "outputs": [],
   "source": [
    "class FeaturesUppercase(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, feature_names, drop_feature_names):\n",
    "        '''\n",
    "        This class is to change feature values to uppercase.\n",
    "        '''\n",
    "        self.feature_names      = feature_names\n",
    "        self.drop_feature_names = drop_feature_names\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "        \n",
    "    def transform(self, X, y=None):\n",
    "        '''\n",
    "        This method is to change feature values to uppercase.\n",
    "        '''\n",
    "        \n",
    "        func = lambda x: x.strip().upper()\n",
    "        \n",
    "        X1 = X.copy()\n",
    "        \n",
    "        for fname in self.feature_names:\n",
    "            values = X1[fname]\n",
    "            values = values.fillna('NaN')\n",
    "            if type(X1) == ks.DataFrame: \n",
    "                values = transformColumn(values, func, str)\n",
    "            elif type(X1) == pd.DataFrame:\n",
    "                values = map(lambda x: x.strip().upper(), values)\n",
    "            else:\n",
    "                print('FeaturesUppercase: unsupported dataframe: {}'.format(type(X1)))   \n",
    "            X1[fname] = values\n",
    "            \n",
    "        # drop less important features\n",
    "        X1 = X1.drop(self.drop_feature_names, axis=1)\n",
    "            \n",
    "        return X1   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.6 One-Hot Encoding Categorical Values\n",
    "\n",
    "The transform() method of the OneHotEncodeData class calls the get_dummies() method of Koalas dataframe to one-hot encode the values of categorical values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "glg34AKsbpTg"
   },
   "outputs": [],
   "source": [
    "# https://www.guru99.com/pyspark-tutorial.html\n",
    "class OneHotEncodeData(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        '''\n",
    "        This class is to one-hot encode the categorical features.\n",
    "        '''\n",
    "        self.one_hot_feature_names = ['Client name', \n",
    "                        'Industry', \n",
    "                        'Location', \n",
    "                        'Position to be closed', \n",
    "                        'Nature of Skillset',\n",
    "                        'Interview Type', \n",
    "                        #'Name(Cand ID)', \n",
    "                        'Gender', \n",
    "                        'Candidate Current Location',\n",
    "                        'Candidate Job Location', \n",
    "                        'Interview Venue', \n",
    "                        'Candidate Native location',\n",
    "                        'Have you obtained the necessary permission to start at the required time',\n",
    "                        'Hope there will be no unscheduled meetings',\n",
    "                        'Can I Call you three hours before the interview and follow up on your attendance for the interview',\n",
    "                        'Can I have an alternative number/ desk number. I assure you that I will not trouble you too much',\n",
    "                        'Have you taken a printout of your updated resume. Have you read the JD and understood the same',\n",
    "                        'Are you clear with the venue details and the landmark.',\n",
    "                        'Has the call letter been shared', \n",
    "                        'Marital Status']\n",
    "        self.label_encoders   = None\n",
    "        self.one_hot_encoders = None\n",
    "        \n",
    "    def fit(self, X, y=None):       \n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, y=None):  \n",
    "        X1 = X.copy()\n",
    "        if type(X1) == ks.DataFrame: \n",
    "            X1 = ks.get_dummies(X1)\n",
    "        elif type(X1) == pd.DataFrame:\n",
    "            X1 = pd.get_dummies(X1)\n",
    "        else:\n",
    "            print('OneHotEncodeData: unsupported dataframe: {}'.format(type(X1)))\n",
    "            pass\n",
    "        \n",
    "        return X1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Modeling\n",
    "\n",
    "Once the dataset has been prepared, the next step is modeling. The main goals of modeling are:\n",
    "* Identify machine learning algorithm\n",
    "* Create and train machine learning model\n",
    "* Tune the hyperparameters of machine learning algorithm\n",
    "\n",
    "In this article we use the scikit-learn RandomForestClassifier algorithm for demonstration purpose. Grid search is used to tune the hyperparameters (number of estimators and max depth) of the algorithm, and mlflow is used to train model, track, and compare the performance metrics of different models. \n",
    "\n",
    "The GridSearch class is to implement grid search and the method of mlFlow() of the PredictInterview class is to call other methods to train model, use a trained model to predict results, and record model performance metrocs using mlflow API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DN6TnUV8eaSe"
   },
   "outputs": [],
   "source": [
    "class GridSearch(object):\n",
    "    def __init__(self, cv=10):\n",
    "        '''\n",
    "        This class finds the best model via Grid Search.\n",
    "        '''\n",
    "        self.grid_param = [\n",
    "            {'n_estimators': range(68,69), # range(60, 70) # best 68\n",
    "             'max_depth'   : range(8,9)}   # range(5, 10)}  # best 8\n",
    "        ]\n",
    "        self.cv = cv\n",
    "        self.scoring_function = make_scorer(f1_score, greater_is_better=True) \n",
    "        self.gridSearch = None\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        rfc = RandomForestClassifier()\n",
    "        self.gridSearchCV = GridSearchCV(rfc, self.grid_param, cv=self.cv, scoring=self.scoring_function)\n",
    "        self.gridSearchCV.fit(X, y)\n",
    "        return self.gridSearchCV.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "P9I-2iIeehfI"
   },
   "outputs": [],
   "source": [
    "class PredictInterview(object):\n",
    "    def __init__(self, dataset_file=dataset_file, use_koalas=True):\n",
    "        '''\n",
    "        This class is to predict the probability of a candidate attending scheduled interviews.\n",
    "        '''\n",
    "        self.use_koalas = use_koalas\n",
    "        self.dataset_file_name = dataset_file\n",
    "        self.feature_names = ['Date of Interview', \n",
    "                       'Client name', \n",
    "                       'Industry', \n",
    "                       'Location', \n",
    "                       'Position to be closed', \n",
    "                       'Nature of Skillset',\n",
    "                       'Interview Type', \n",
    "                       #'Name(Cand ID)',\n",
    "                       'Gender', \n",
    "                       'Candidate Current Location',\n",
    "                       'Candidate Job Location', \n",
    "                       'Interview Venue', \n",
    "                       'Candidate Native location',\n",
    "                       'Have you obtained the necessary permission to start at the required time',\n",
    "                       'Hope there will be no unscheduled meetings',\n",
    "                       'Can I Call you three hours before the interview and follow up on your attendance for the interview',\n",
    "                       'Can I have an alternative number/ desk number. I assure you that I will not trouble you too much',\n",
    "                       'Have you taken a printout of your updated resume. Have you read the JD and understood the same',\n",
    "                       'Are you clear with the venue details and the landmark.',\n",
    "                       'Has the call letter been shared', 'Marital Status']\n",
    "        \n",
    "        if self.use_koalas:\n",
    "            self.drop_feature_names = [\n",
    "                'Name(Cand ID)',\n",
    "                'Date of Interview', \n",
    "                '_c22',\n",
    "                '_c23',\n",
    "                '_c24',\n",
    "                '_c25',\n",
    "                '_c26']\n",
    "        else: # use Pandas\n",
    "            self.drop_feature_names = [\n",
    "                'Unnamed: 22',\n",
    "                'Unnamed: 23',\n",
    "                'Unnamed: 24',\n",
    "                'Unnamed: 25',\n",
    "                'Unnamed: 26']\n",
    "        \n",
    "        self.dataset = None\n",
    "        self.rfc     = None\n",
    "        self.gridSearch = None\n",
    "        self.X_train = None\n",
    "        self.y_train = None\n",
    "        self.X_test  = None\n",
    "        self.y_test  = None\n",
    "        self.y_pred  = None\n",
    "        self.X_clean = None\n",
    "        self.y_clean = None\n",
    "        self.X_train_encoded = None\n",
    "        self.X_test_encoded  = None\n",
    "        self.y_train_encoded = None\n",
    "        self.accuracy_score  = None \n",
    "        self.f1_score        = None\n",
    "        self.oneHotEncoder   = None\n",
    "        self.X_test_name_ids = None\n",
    "        self.pipeline = None\n",
    "        \n",
    "        \n",
    "    def loadData(self, path=None):\n",
    "        '''\n",
    "        This method loads a dataset file as a Pandas DataFrame, assuming that the dataset file is in csv format.\n",
    "        It also shuffles the loaded dataset as part of data preprocessing.\n",
    "        '''\n",
    "        if (path != None):\n",
    "            path = os.path.join(path, self.dataset_file_name)\n",
    "        else:\n",
    "            path = self.dataset_file_name\n",
    "         \n",
    "        if self.use_koalas:\n",
    "            dataset = ks.read_csv(path)  \n",
    "        else:\n",
    "            dataset = pd.read_csv(path)\n",
    "        \n",
    "        # shuffle data \n",
    "        self.dataset = dataset.sample(frac=1.0) \n",
    "        \n",
    "        return self.dataset     \n",
    "    \n",
    "    def PreprocessData(self):\n",
    "        '''\n",
    "        This method preprocesses the loaded dataset before applying one-hot encoding.\n",
    "        '''\n",
    "            \n",
    "        y = self.dataset['Observed Attendance']      # extract labels y\n",
    "        if self.use_koalas:\n",
    "            X = self.dataset.drop('Observed Attendance') # extract features X\n",
    "        else:\n",
    "            X = self.dataset.drop(['Observed Attendance'], axis=1) \n",
    "        \n",
    "        self.oneHotEncoder = OneHotEncodeData()\n",
    "        \n",
    "        self.pipeline = Pipeline([\n",
    "            ('bucket_skillset', BucketSkillset()),\n",
    "            ('bucket_location', BucketLocation()),\n",
    "            ('parse_interview_date', ParseInterviewDate()),\n",
    "            ('features_to_uppercase', FeaturesUppercase(self.feature_names, self.drop_feature_names)),\n",
    "            ('one_hot_encoder', self.oneHotEncoder)\n",
    "        ])\n",
    "        \n",
    "        X_1hot = self.pipeline.fit_transform(X)\n",
    "        \n",
    "        # fill up missing labels and then change labels to uppercase\n",
    "        y = y.fillna('NaN')\n",
    "        \n",
    "        if self.use_koalas:\n",
    "            func = lambda x: x.strip().upper()\n",
    "            y_uppercase = transformColumn(y, func, str) \n",
    "        else:\n",
    "            y_uppercase = map(lambda x: x.strip().upper(), y.values)\n",
    "            y_uppercase = pd.Series(y_uppercase)\n",
    "        \n",
    "        # separate labeled records from unlabeled records\n",
    "        self.X_train_encoded = X_1hot[y_uppercase != 'NAN']\n",
    "        self.X_test_encoded  = X_1hot[y_uppercase == 'NAN']\n",
    "        \n",
    "        # save Names/ID for reporting later one\n",
    "        self.X_test_name_ids = self.dataset['Name(Cand ID)'].loc[y_uppercase == 'NAN']\n",
    "        \n",
    "        y_train = y_uppercase.loc[y_uppercase != 'NAN']\n",
    "        \n",
    "        # encode labels as follows: 0 - NO, 1 - YES, NAN - NAN\n",
    "        if self.use_koalas:\n",
    "            func = lambda x: 1 if x == 'YES' else 0\n",
    "            y = transformColumn(y_train, func, int)\n",
    "        else:\n",
    "            y = map(lambda x: 1 if x == 'YES' else 0, y_train)\n",
    "            y = pd.Series(y)\n",
    "        \n",
    "        self.y_train_encoded = y\n",
    "        \n",
    "        self.X_clean = X_1hot\n",
    "        self.y_clean = y_uppercase\n",
    "        \n",
    "        return None\n",
    "    \n",
    "    def __splitData(self):\n",
    "        '''\n",
    "        This method triggers data preprocsssing and split dataset into training and testing datasets.\n",
    "        '''\n",
    "        if self.use_koalas:\n",
    "            X_train_encoded = self.X_train_encoded.to_numpy()\n",
    "            y_train_encoded = self.y_train_encoded.to_numpy()\n",
    "        else:\n",
    "            X_train_encoded = self.X_train_encoded.values\n",
    "            y_train_encoded = self.y_train_encoded.values\n",
    "            \n",
    "        self.X_train, self.X_test, self.y_train, self.y_test = train_test_split(X_train_encoded, \n",
    "                                                                                y_train_encoded, \n",
    "                                                                                test_size = 0.25, random_state = 0)\n",
    "        return (self.X_train, self.X_test, self.y_train, self.y_test)\n",
    "    \n",
    "    def trainModel(self):\n",
    "        '''\n",
    "        This method triggers splitting dataset and then find a best RandomForest model via grid search \n",
    "        using the training features and labels.\n",
    "        '''\n",
    "        X_train, X_test, y_train, y_test = self.__splitData()\n",
    "        self.gridSearch = GridSearch()\n",
    "        self.rfc = self.gridSearch.fit(X_train, y_train)\n",
    "        return self.rfc\n",
    "    \n",
    "    def predictClasses(self):\n",
    "        '''\n",
    "        This method predicts classes (YES or NO) using a trained model.\n",
    "        '''\n",
    "        if (self.rfc is None):\n",
    "            print(\"No trained model available, please train a model first!\")\n",
    "            return None\n",
    "        \n",
    "        self.y_pred = self.rfc.predict(self.X_test)\n",
    "        return self.y_pred\n",
    "    \n",
    "    def getModelMetrics(self):\n",
    "        '''\n",
    "        This method obtains the class prediction scores: (Accuracy Score, R2, F1).\n",
    "        '''\n",
    "        if (self.y_test is None or self.y_pred is None):\n",
    "            print('Failed to get model performance metrics because y_test is null or y_pred is null!')\n",
    "            return None\n",
    "        \n",
    "        self.accuracy_score = accuracy_score(self.y_test, self.y_pred)\n",
    "        self.f1_score = f1_score(self.y_test, self.y_pred)\n",
    "        \n",
    "        pred = self.predictAttendanceProbability(self.X_test)[:, 1]\n",
    "        actual = self.y_test.astype(float)\n",
    "        \n",
    "        self.rmse_score = np.sqrt(mean_squared_error(actual, pred))\n",
    "        self.mae_score = mean_absolute_error(actual, pred)\n",
    "        self.r2_score = r2_score(actual, pred)\n",
    "        \n",
    "        return (self.accuracy_score, self.f1_score, self.rmse_score, self.mae_score, self.r2_score)\n",
    "    \n",
    "    def predictNullAttendanceProbability(self):\n",
    "        '''\n",
    "        This method uses a trained model to predict the attendance probability for \n",
    "        the candidates where the \"Observed Attendance\" column is null.\n",
    "        '''\n",
    "        y_pred = self.rfc.predict_proba(self.X_test_encoded.to_numpy())\n",
    "        return y_pred\n",
    "    \n",
    "    def predictNullAttendanceClasses(self):\n",
    "        '''\n",
    "        This method predicts classes (YES or NO) using a trained model for unlabeled data records.\n",
    "        '''\n",
    "        y_pred = self.rfc.predict(self.X_test_encoded.to_numpy())\n",
    "        return y_pred\n",
    "    \n",
    "    def predictAttendanceProbability(self, X):\n",
    "        '''\n",
    "        Given one preprocessed (including one-hot encoding) data smaple X,\n",
    "        this method returns the probability of attendance probability.\n",
    "        '''\n",
    "        y_pred = self.rfc.predict_proba(X)\n",
    "        return y_pred\n",
    "    \n",
    "    def predictAttendanceClass(self, X):\n",
    "        '''\n",
    "        Given one preprocessed (including one-hot encoding) data smaple X,\n",
    "        this method returns the attendance Yes/No.\n",
    "        '''\n",
    "        y_pred = self.rfc.predict(X)\n",
    "        return y_pred\n",
    "    \n",
    "    def mlFlow(self):\n",
    "        '''\n",
    "        Training model in mlflow\n",
    "        * https://www.mlflow.org/docs/latest/tutorial.html\n",
    "        '''\n",
    "        np.random.seed(40)\n",
    "        with mlflow.start_run():\n",
    "            self.loadData()\n",
    "            self.PreprocessData()\n",
    "            self.trainModel()\n",
    "            self.predictClasses()\n",
    "            accuracy_score, f1_score, rmse_score, mae_score, r2_score = self.getModelMetrics()\n",
    "\n",
    "            print(\"Random Forest model:\")\n",
    "            print(\"  RMSE: {}\".format(rmse_score))\n",
    "            print(\"  MAE: {}\".format(mae_score))\n",
    "            print(\"  R2: {}\".format(r2_score))\n",
    "            print(\"Accuracy Score: {}\".format(accuracy_score))\n",
    "            print(\"  f1: {}\".format(f1_score))\n",
    "            \n",
    "            best_params = self.gridSearch.gridSearchCV.best_params_ \n",
    "\n",
    "            mlflow.log_param(\"n_estimators\", best_params[\"n_estimators\"])\n",
    "            mlflow.log_param(\"max_depth\", best_params[\"max_depth\"])\n",
    "            mlflow.log_metric(\"rmse\", rmse_score)\n",
    "            mlflow.log_metric(\"r2\", r2_score)\n",
    "            mlflow.log_metric(\"mae\", mae_score)\n",
    "            mlflow.log_metric(\"accuracy\", accuracy_score)\n",
    "            mlflow.log_metric(\"f1\", f1_score)\n",
    "\n",
    "            mlflow.sklearn.log_model(self.rfc, \"random_forest_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YGJCZPyCeuiZ"
   },
   "source": [
    "Task 1 \n",
    "\n",
    "(a) Create a model predicting if a candidate will attend an interview. This will be indicated by the \"Observed Attendance\" column in the data set. Create the model only using the records where this column is not null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "8cn6g-Xgeoz5",
    "outputId": "b34e1965-2b48-4983-e181-41b0510c2892"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest model:\n",
      "  RMSE: 0.4222949212871352\n",
      "  MAE: 0.3818960214247194\n",
      "  R2: 0.1360433037540183\n",
      "Accuracy Score: 0.7298245614035088\n",
      "  f1: 0.8237986270022882\n"
     ]
    }
   ],
   "source": [
    "predictInterview = PredictInterview(use_koalas=True)\n",
    "predictInterview.mlFlow()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Evaluation\n",
    "\n",
    "Once a machine learning model is trained with expected performance, the next step is to assess the prediction results of the model in a controlled close-to-real settings to gain confidence that the model is valid, reliable, and meets business requirements before deployment.\n",
    "\n",
    "As an example, for the Kaggle interview attendance project, one possible method of evaluation is to use mlflow to deploy the model as a Web service locally and then develop a client program to call the model Web service to get the prediction results of a testing dataset processed by data preparation. These prediction results can then be used to generate a table or csv file for recruitment industry domain experts to review.\n",
    "\n",
    "The code below obtains a probability and a prediction for each of the candidates where the \"Observed Attendance\" column is null."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "3xVjeQ9jfKTO",
    "outputId": "138e594d-4c09-4525-e4b0-02f17a567f80"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Names/ID</th>\n",
       "      <th>Probability</th>\n",
       "      <th>Yes/No</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Candidate 10</td>\n",
       "      <td>0.808386</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Candidate 20</td>\n",
       "      <td>0.868603</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Candidate 30</td>\n",
       "      <td>0.651773</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Candidate 40</td>\n",
       "      <td>0.68215</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Candidate 50</td>\n",
       "      <td>0.656323</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Candidate 60</td>\n",
       "      <td>0.666368</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Candidate 70</td>\n",
       "      <td>0.666368</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Candidate 80</td>\n",
       "      <td>0.656323</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Candidate 90</td>\n",
       "      <td>0.134323</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Candidate 100</td>\n",
       "      <td>0.648787</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Candidate 110</td>\n",
       "      <td>0.648787</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Candidate 120</td>\n",
       "      <td>0.71933</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Candidate 130</td>\n",
       "      <td>0.648787</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Candidate 140</td>\n",
       "      <td>0.886644</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Candidate 150</td>\n",
       "      <td>0.886644</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Names/ID Probability Yes/No\n",
       "0    Candidate 10    0.808386    yes\n",
       "1    Candidate 20    0.868603    yes\n",
       "2    Candidate 30    0.651773    yes\n",
       "3    Candidate 40     0.68215    yes\n",
       "4    Candidate 50    0.656323    yes\n",
       "5    Candidate 60    0.666368    yes\n",
       "6    Candidate 70    0.666368    yes\n",
       "7    Candidate 80    0.656323    yes\n",
       "8    Candidate 90    0.134323     no\n",
       "9   Candidate 100    0.648787    yes\n",
       "10  Candidate 110    0.648787    yes\n",
       "11  Candidate 120     0.71933    yes\n",
       "12  Candidate 130    0.648787    yes\n",
       "13  Candidate 140    0.886644    yes\n",
       "14  Candidate 150    0.886644    yes"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_probs   = predictInterview.predictNullAttendanceProbability()\n",
    "pred_classes = predictInterview.predictNullAttendanceClasses()\n",
    "\n",
    "x = predictInterview.X_test_name_ids.to_numpy() \n",
    "z = zip(x, pred_probs, pred_classes)\n",
    "answers = ('no', 'yes')\n",
    "\n",
    "result = [[x1, p1[1], answers[c]] for x1, p1, c in z]\n",
    "result_df = pd.DataFrame(np.array(result), columns=['Names/ID', 'Probability', 'Yes/No'])\n",
    "result_df.to_csv('./data/interview_prediction_results.csv')\n",
    "result_df.head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start mlflow UI\n",
    "# ! mlflow ui"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Year</th>\n",
       "      <th>Month</th>\n",
       "      <th>Day</th>\n",
       "      <th>Client name_ANZ</th>\n",
       "      <th>Client name_AON HEWITT</th>\n",
       "      <th>Client name_AON HEWITT GURGAON</th>\n",
       "      <th>Client name_ASTRAZENECA</th>\n",
       "      <th>Client name_BARCLAYS</th>\n",
       "      <th>Client name_FLEXTRONICS</th>\n",
       "      <th>Client name_HEWITT</th>\n",
       "      <th>...</th>\n",
       "      <th>Has the call letter been shared_NAN</th>\n",
       "      <th>Has the call letter been shared_NEED TO CHECK</th>\n",
       "      <th>Has the call letter been shared_NO</th>\n",
       "      <th>Has the call letter been shared_NOT SURE</th>\n",
       "      <th>Has the call letter been shared_NOT YET</th>\n",
       "      <th>Has the call letter been shared_YES</th>\n",
       "      <th>Has the call letter been shared_YET TO CHECK</th>\n",
       "      <th>Marital Status_MARRIED</th>\n",
       "      <th>Marital Status_NAN</th>\n",
       "      <th>Marital Status_SINGLE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2016</td>\n",
       "      <td>4</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows  167 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Year  Month  Day  Client name_ANZ  Client name_AON HEWITT  \\\n",
       "0  2016      4   21                0                       0   \n",
       "\n",
       "   Client name_AON HEWITT GURGAON  Client name_ASTRAZENECA  \\\n",
       "0                               0                        0   \n",
       "\n",
       "   Client name_BARCLAYS  Client name_FLEXTRONICS  Client name_HEWITT  ...  \\\n",
       "0                     0                        0                   0  ...   \n",
       "\n",
       "   Has the call letter been shared_NAN  \\\n",
       "0                                    0   \n",
       "\n",
       "   Has the call letter been shared_NEED TO CHECK  \\\n",
       "0                                              0   \n",
       "\n",
       "   Has the call letter been shared_NO  \\\n",
       "0                                   0   \n",
       "\n",
       "   Has the call letter been shared_NOT SURE  \\\n",
       "0                                         0   \n",
       "\n",
       "   Has the call letter been shared_NOT YET  \\\n",
       "0                                        0   \n",
       "\n",
       "   Has the call letter been shared_YES  \\\n",
       "0                                    0   \n",
       "\n",
       "   Has the call letter been shared_YET TO CHECK  Marital Status_MARRIED  \\\n",
       "0                                             0                       1   \n",
       "\n",
       "   Marital Status_NAN  Marital Status_SINGLE  \n",
       "0                   0                      0  \n",
       "\n",
       "[1 rows x 167 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "columns = predictInterview.X_train_encoded.columns\n",
    "data1 = predictInterview.X_test[1]\n",
    "test_df1 = pd.DataFrame(data1.reshape((1,-1)))\n",
    "test_df1.columns = columns\n",
    "test_df = test_df1.loc[:2]\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_json_obj = test_df.to_json(orient='split')\n",
    "# data_json_obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "headers = {'Content-Type': 'application/json',\n",
    "           'Format': 'pandas-split'}\n",
    "url = 'http://127.0.0.1:1234/invocations'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers_json_str = json.dumps(headers)\n",
    "headers_json_obj = json.loads(headers_json_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[0]'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = requests.post(url, data=data_json_obj, headers = headers_json_obj)\n",
    "response.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Deployment\n",
    "\n",
    "Once the model evaluation includes that the model is ready for deployment, the final step is to deploy an evaluated model into a production system. As described in [Data Science for Business](http://data-science-for-biz.com/), the specifics of deployment depend on the target production system. \n",
    "\n",
    "Taking the Kaggle interview attendance project as an example, one possible scenario is to deploy the model as a Web service on a server, which can be called by other components in a target production system to get prediction results. In a more complicated scenario that the development of the target production system is based on a programming language that is different from the modeling lanuage (e.g., Python), then the chance is that the model needs to be reimplemented in the target programming language as a component of the production system.  \n",
    "\n",
    "As described before, mlflow has built-in capability of starting a logged model as a Web service:\n",
    "\n",
    "mlflow models serve -m /Users/yuhuang/yuefeng/machine-learning-spark/mlruns/0/258301f3ac5f42fb99e885968ff17c2a/artifacts/random_forest_model -p 1234"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this article, I use a close-to-real challenging dataset, the [Interview Attendance Problem for Kaggle competition](https://www.kaggle.com/vishnusraghavan/the-interview-attendance-problem), to demonstrate an end-to-end process of developing a machine learning pipeline for both Pandas and Koalas dataframes by combining Pandas and Koalas with Scikit-Learn and mlflow. This end-to-end development process follows the [Cross-industry standard process for data mining](https://en.wikipedia.org/wiki/Cross-industry_standard_process_for_data_mining). A brief description for each of the major phases of the standard process is provided. "
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "interview_attendance.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
